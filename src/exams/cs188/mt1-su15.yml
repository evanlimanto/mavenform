course: 'cs188'
ref: 'mt1-su15'

type: 'mt1'
term: 'su15'
prof: '-'

questions: {
  'q1': '',
  'q2': '',
  'q3': '',
  'q4': '',
}

parts: {
  'q1': 11,
  'q2': 3,
  'q3': 3,
  'q4': 3,
  'q5': 8,
}

q1_1: |
  ## Q1. [12 pts] Search
  Each True/False question is worth 1 points. Leaving a question blank is worth 0 points. **Answering incorrectly is worth −1 points.**
  <hr class="s2" />
  (a) Consider a graph search problem where for every action, the cost is at least $\varepsilon$, with $ε > 0$. Assume the used heuristic is consistent.
  <hr class="s2" />
  **(i)** [1 pt] **[true or false]** Depth-first graph search is guaranteed to return an optimal solution.

q1_1_s: |
  **False**. Depth first search has no guarantees of optimality. Further, it measures paths in length and not cost.

q1_2: |
  **(ii)** [1 pt] **[true or false]** Breadth-first graph search is guaranteed to return an optimal solution.

q1_2_s: |
 **False**. Breadth first search has no guarantees of optimality unless the actions all have the same cost, which is not the case here.

q1_3: |
 **(iii)** [1 pt] **[true or false]** Uniform-cost graph search is guaranteed to return an optimal solution.

q1_3_s: |
  **True**. UCS expands paths in order of least total cost so that the optimal solution is found.

q1_4: |
 **(iv)** [1 pt] **[true or false]** Greedy graph search is guaranteed to return an optimal solution.

q1_4_s: |
 **False**. Greedy search makes no guarantees of optimality. It relies solely on the heuristic and not the true cost.

q1_5: |
 **(v)** [1 pt] **[true or false]** $A^*$ graph search is guaranteed to return an optimal solution.

q1_5_s: |
  **True**, since the heuristic is consistent in this case.

q1_6: |
 **(vi)** [1 pt] **[true or false]** $A^*$ graph search is guaranteed to expand no more nodes than depth-first graph search.

q1_6_s: |
 **False**. Depth-first graph search could, for example, go directly to a sub-optimal solution.

q1_7: |
 **(vii)** [1 pt] **[true or false]** $A^*$ graph search is guaranteed to expand no more nodes than uniform-cost graph search.

q1_7_s: |
 **True**. The heuristic could help to guide the search and reduce the number of nodes expanded. In the extreme case where the heuristic function returns zero for every state, $A^\*$ and UCS will expand the same number of nodes. In any case, $A^*$ with a consistent heuristic will never expand more nodes than UCS.

q1_8: |
  **(b)** Let $h_1(s)$ be an admissible $A^*$ heuristic. Let $h_2(s) = 2h_1(s)$. Then:
  <hr class="s1" />
  **(i)** [1 pt] **[true or false]** The solution found by $A^*$ tree search with $h_2$ is guaranteed to be an optimal solution.

q1_8_s: |
  **False**. $h_2$ is not guaranteed to be admissible since only one side of the admissibility inequality is doubled.

q1_9: |
  **(ii)** [1 pt] **[true or false]** The solution found by $A^*$ tree search with $h_2$ is guaranteed to have a cost at most twice as much as the optimal path.

q1_9_s: |
  **True**. In $A^*$ tree search we always have that as long as the optimal path to the goal has not been found, a prefix of this optimal path has to be on the fringe. Hence, if a non-optimal solution is found, then at time of popping the non-optimal path from the fringe, a path that is a prefix of the optimal path to the goal is sitting on the fringe. The cost $\bar{g}$ of a non-optimal solution when popped is its f-cost. The prefix of the optimal path to the goal has an f-cost of $g + h_0 = g + 2h_1 ≤ 2(g+h_1) ≤ 2C^∗$, with $C^∗$ the optimal cost to the goal. Hence we have that $\bar{g} ≤ 2C^∗$ and the found path is at most twice as long as the optimal path.

q1_10: |
  **(iii)** [1 pt] **[true or false]** The solution found by $A^*$ graph search with $h_2$ is guaranteed to be an optimal solution.

q1_10_s: |
  **False**. $h_2$ is not guaranteed to be admissible and graph search further requires consistency for optimality.

q1_11: |
  **(c)** [2 pts] The heuristic values for the graph below are not correct. For which single state ($S, A, B, C, D, \text{ or } G$) could you change the heuristic value to make everything admissible and consistent? What range of values are possible to make this correction?
  <hr class="s2" />
  ![graph](/img/cs188/mt1-su15-q1-1.png)
  <hr class="s2" />
  - State: ________
  - Range: ________

q1_11_s: |
  - State: $B$
  - Range: $[2,3]$

q2_1: |
  ## Q2. [9 pts] Game Trees and Pruning
  You and one of the $188$ robots are playing a game where you both have your own score.
  - The maximum possible score for either player is $10$.
  - You are trying to maximize your score, and you do not care what score the robot gets.
  - The robot is trying to minimize the absolute difference between the two scores. In the case of a tie, the robot prefers a lower score. For example, the robot prefers $(5,3)$ to $(6,3)$; it prefers $(5,3)$ to $(0,3)$; and it prefers $(3,3)$ to $(5,5)$.

  <hr class="s2" />
  The figure below shows the game tree of your max node followed by the robots nodes for your four different actions. The scores are shown at the leaf nodes with your score always on top and the robots score on the bottom.
  <hr class="s2" />
  **(a)** [3 pts] Fill in the dashed rectangles with the pair of scores preferred by each node of the game tree.
  <hr class="s2" />
  ![game tree](/img/cs188/mt1-su15-q2-1.png)

q2_1_s: |
  See solution of part (b).

q2_2: |
  (b) [3 pts] You can save computation time by using pruning in your game tree search. On the game tree above, put an ’X’ on line of branches that do not need to be explored. Assume that branches are explored from left to right.

q2_2_s: |
  ![game tree](/img/cs188/mt1-su15-q2-2.png)

q2_3: |
  **(c)** [2 pts] You now have access to an oracle that tells you the order of branches to explore that maximizes pruning. On the copy of the game tree below, put an ’X’ on line of branches that do not need to be explored given this new information from the oracle.
  <hr class="s2" />
  ![game tree](/img/cs188/mt1-su15-q2-3.png)

q2_3_s: |
  ![game tree](/img/cs188/mt1-su15-q2-4.png)

q3_1: |
  ## Q3. [6 pts] Encrypted Knowledge Base
  We have a propositional logic knowledge base, but unfortunately, it is encrypted. The only information we have is that:
  - Each of the following $12$ boxes contains a propositional logic symbol ($A, B, C, D,$ or E$) or a propositional logic operator and
  - Each line is a valid propositional logic sentence.

  <hr class="s2" />
  <p align="center">![knowledge base](/img/cs188/mt1-su15-q3-1.png)</p>
  <hr class="s2" />
  **(a)** [3 pts] We are going to implement a constraint satisfaction problem solver to find a valid assignment to each box from the domain $\\{A, B, C, D, E, ∧, ∨, ¬, ⇒, ⇔\\}$.
  <hr class="s2" />
  Propositional logic syntax imposes constraints on what can go in each box. What values are in the domain of boxes 1-6 after enforcing the unary syntax constraints?
  <hr class="s2" />
  <p align="center">![knowledge base](/img/cs188/mt1-su15-q3-2.png)</p>

q3_1_s: |
  ![table](/img/cs188/mt1-su15-q3-3.png)

q3_2: |
  **(b)** [2 pts] You are given the following assignment as a solution to the knowledge base CSP on the previous page:
  <hr class="s2" />
  <p align="center">![knowledge base](/img/cs188/mt1-su15-q3-4.png)</p>
  <hr class="s2" />
  Now that the encryption CSP is solved, we have an entirely new CSP to work on: finding a model. In this new CSP the variables are the symbols $\\{A, B, C, D, E\\}$ and each variable could be assigned to true or false.
  <hr class="s2" />
  We are going to run CSP backtracking search with forward checking to find a propositional logic model $M$ that makes all of the sentences in this knowledge base true.
  <hr class="s2" />
  After choosing to assign $C$ to false, what values are removed by running forward checking? On the table of remaining values below, cross off the values that were removed.
  <hr class="s2" />
  ![knowledge base](/img/cs188/mt1-su15-q3-5.png)

q3_2_s: |
  ![knowledge base](/img/cs188/mt1-su15-q3-6.png)
  <hr class="s2" />
  Forward checking removes the value false from the domain of $B$. Forward checking does not continue on to make any other arcs consistent.

q3_3: |
  **(c)** [2 pts] We eventually arrive at the model $M = \\{A = False, B = False, C = True, D = True, E = True\\}$ that causes all of the knowledge base sentences to be true. We have a query sentence $α$ specific as $(A ∨ C) \implies E$. Our model $M$ also causes $\alpha$ to be true. Can we say that the knowledge base entails $α$? Explain briefly (in one sentence) why or why not.

q3_3_s: |
  No, the knowledge base does not entail $α$. There are other models for which the knowledge base could be true and the query be false. Specifically $\\{A = False, B = False, C = True, D = True, E = False\\}$ satisfies the knowledge base but causes the query $α$ to be false.

q4_1: |
  ## Q4. [8 pts] MDPs - Farmland
  In the game FARMLAND, players alternate taking turns drawing a card from one of two piles, PIG and COW. PIG cards have equal probability of being $3$ points or $1$ point, and COW cards are always worth $2$ points. Players are trying to be the first to reach $5$ points or more. We are designing an agent to play FARMLAND.
  <hr class="s2" />
  We will use a modified MDP to come up with a policy to play this game. States will be represented as tuples $(x, y)$ where $x$ is our score and $y$ is the opponent’s score. The value $V(x, y)$ is an estimate of the probability that we will win at the state $(x,y)$ **when it is our turn to play** and both players are playing optimally. Unless otherwise stated, assume both players play optimally.
  <hr class="s2" />
  First, suppose we work out by hand $V^∗$, the table of actual probabilities.
  <hr class="s2" />
  <p align="center">![table](/img/cs188/mt1-su15-q4-1.png)</p>
  <hr class="s2" />
  According to this table, $V^∗(1, 2) = 0.5$, so with both players playing optimally, the probability that we will win if our score is $1$, the opponent’s score is $2$, and it is our turn to play is $0.5$.
  <hr class="s2" />
  **(a)** [2 pts] At the beginning of the game, would you choose to go first or second? Justify your answer using the table.

q4_1_s: |
  You should choose to go first. Since $V^∗(0, 0) = 0.75$, if it is your turn and the scores are both $0$, the probability that you will win is $0.75$.

q4_2: |
  **(b)** [3 pts] If our current state is $(x, y)$ (so our score is $x$ and the opponent’s score is $y$) but it is **the opponent’s turn to play**, what is the probability that we will win if both players play optimally **in terms of $V^∗$**?

q4_2_s: |
  $$1 - V^*(y, x)$$

q4_3: |
  **(c)** [3 pts] As FARMLAND is a very simple game, you quickly grow tired of playing it. You decide to buy the FARMLAND expansion, BOVINE BONANZA, which adds loads of exciting cards to the COW pile! Of course, this changes the transition function for our MDP, so the table $V^∗$ above is no longer correct. We need to come up with an update equation that will ultimately make $V_{\infty}$ converge on the actual probabilities that we will win.
  <hr class="s2" />
  You are given the transition function $T((x, y), a, (x', y))$ and the reward function $R((x, y), a, (x', y))$. The transition function $T ((x, y), a, (x', y))$ is the probability of transitioning from state $(x, y)$ to state $(x', y)$ when action $a$ is taken (i.e. the probability that the card drawn gives $x' − x$ points).
  <hr class="s2" />
  Since we are only trying to find the probability of winning and we don’t care about the margin of victory, the reward function $R((x, y), a, (x', y))$ is $1$ whenever $(x', y)$ is a winning state and $0$ everywhere else. As in normal value iteration, all values will be initialized to $0$ (i.e. $V (x, y) = 0$ for all states $(x, y)$).
  <hr class="s2" />
  Write an update equation for $V_{k+1}(x, y)$ in terms of $T, R$ and $V_k$.
  <hr class="s1" />
  **Hint:** you will need to use your answer from part b.

q4_3_s: |
  $$V_{k+1}(x, y) \leftarrow \max_{a} \sum_{x'} T((x,y),a,(x',y))[R((x,y),a,(x',y)) + (1 - V_{k}(y,x'))]$$

q5_1: |
  ## Q5. [9 pts] Reinforcement Learning
  **(a)** Each True/False question is worth 1 point. Leaving a question blank is worth 0 points. **Answering incorrectly is worth -1 points.**
  <hr class="s2" />
  **(i)** [1 pt] **[true or false]** Temporal difference learning is an online learning method.

q5_1_s: |
  **True.** Temporal difference learning is used when we don’t have the full MDP model and must collect online samples.

q5_2: |
  **(ii)** [1 pt] **[true or false]** $Q$-learning: Using an optimal exploration function leads to no regret while learning the optimal policy.

q5_2_s: |
  **False.** In order to learn the optimal policy, you must explore, and exploring in general has a non-zero chance of regret.

q5_3: |
  **(iii)** [1 pt] **[true or false]** In a deterministic MDP (i.e. one in which each state / action leads to a single deterministic next state), the $Q$-learning update with a learning rate of $α = 1$ will correctly learn the optimal q-values (assume that all state/action pairs are visited sufficiently often).

q5_3_s: |
  **True.** Remember that the learning rate is only there because we are trying to approximate a summation with a single sample. In a deterministic MDP where $s'$ is the single state that always follows when we take action $a$ in state $s$, we have $Q(s, a) = R(s, a, s') + \max_{a'} Q(s', a')$, which is exactly the update we make.

q5_4: |
  **(iv)** [1 pt] **[true or false]** A small discount (close to $0$) encourages greedy behavior.

q5_4_s: |
  **True.** A discount close to zero will place extremely small values on rewards more than one step away, leading to greedy behavior that looks for immediate rewards.

q5_5: |
  **(v)** [1 pt] **[true or false]** A large, negative living reward (≪ $0$) encourages greedy behavior.

q5_5_s: |
  **True.** A negative living reward adds a penalty for every step taken. If that penalty is large, the agent will prefer to find an exit as soon as possible despite potential rewards on longer paths.

q5_6: |
  **(vi)** [1 pt] **[true or false]** A negative living reward can always be expressed using a discount $< 1$.

q5_6_s: |
  **False.** While both negative living rewards and discounts can encourage similar behavior, they are mathematically different. A discount has a multiplicative effect at each step, whereas a living reward only has an additive effect.

q5_7: |
  **(vii)** [1 pt] **[true or false]** A discount < 1 can always be expressed as a negative living reward.

q5_7_s: |
  **False.** While both negative living rewards and discounts can encourage similar behavior, they are mathematically different. A discount has a multiplicative effect at each step, whereas a living reward only has an additive effect.

q5_8: |
  **(b)** [2 pts] Given the following table of $Q$-values for the state $A$ and the set of actions $\\{Forward, Reverse, Stop\\}$, what is the probability that we will take each action on our next move when we following an $ε$-greedy exploration policy (assuming any random movements are chosen uniformly from all actions)?
  - $Q(A, F orward) = 0.75$
  - $Q(A, Reverse) = 0.25$
  - $Q(A, Stop) = 0.5$

  <hr class="s2" />
  ![table](/img/cs188/mt1-su15-q5-1.png)

q5_8_s: |
  ![table](/img/cs188/mt1-su15-q5-2.png)
