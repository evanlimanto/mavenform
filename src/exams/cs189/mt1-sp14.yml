course: 'cs189'
ref: 'mt1-sp14'

type: 'mt1'
term: 'sp14'
prof: 'Shewchuk'

questions: {
  'q1': 'Multiple Choice',
  'q2': 'Quadratics and Gaussian Isocontours',
  'q3': 'Linear Regression',
  'q4': 'Discriminant Analysis',
}

parts: {
  'q1': 10,
  'q2': 9,
  'q3': 1,
  'q4': 2,
  'q5': 1,
  'q6': 1,
  'q7': 2,
  'q8': 2,
}

q1_1: |
  ## Q1. [10 pts] True or False
  **(a)** [1 pt] The hyperparameters in the regularized logistic regresion model are $η$ (learning rate) and $\lambda$ (regularization term).
  - True
  - False

q1_1_s: |
  - False

q1_2: |
  **(b)** [1 pt] The objective function used in L2 regularized logistic regression is convex.
  - True
  - False

q1_2_s: |
  - True

q1_3: |
  **(c)** [1 pt] In SVMs, the values of $\alpha_i$ for non-support vectors are 0.
  - True
  - False

q1_3_s: |
  - True

q1_4: |
  **(d)** [1 pt] As the number of data points approaches $\infty$, the error rate of a 1-NN classifier approaches 0.
  - True
  - False

q1_4_s: |
  - False

q1_5: |
  **(e)** [1 pt] Cross validation will guarantee that our model does not overfit.
  - True
  - False

q1_5_s: |
  - False

q1_6: |
  **(f)** [1 pt] As the number of dimensions increases, the percentage of the volume in the unit ball shell with thickness $\varepsilon$ grows.
  - True
  - False

q1_6_s: |
  - True

q1_7: |
  **(g)** [1 pt] In logistic regression, the Hessian of the (non-regularized) log likelihood is positive definite.
  - True
  - False

q1_7_s: |
  - False

q1_8: |
  **(h)** [1 pt] Given a binary classification scenario with Gaussian class conditionals and equal prior probabilities, the optimal decision boundary will be linear.
  - True
  - False

q1_8_s: |
  - False

q1_9: |
  **(i)** [1 pt] In the primal version of SVM, we are minimizing the Lagrangian with respect to $w$ and in the dual version, we are minimizing the Lagrangian with respect to $\alpha$.
  - True
  - False

q1_9_s: |
  - False

q1_10: |
  **(j)** [1 pt] For the dual version of soft margin SVM, the $\alpha_i$'s for support vectors specify $\alpha_i > C$.

q1_10_s: |
  - False

q2_1: |
  ## Q2. [24 pts] Multiple Choice
  Fill in the bubbles for **ALL correct choices**: there may be more than one correct choice, but there is always at least one correct choice. **NO partial credit**: the set of all correct answers must be checked.
  <hr class="s1" />
  **(a)** [3 pts] Consider the binary classification problem where $y \in \\{0, 1\\}$ is the label and we have prior probability $P(y = 0) = \pi_0$. If we model $P(x|y = 1)$ to be the following distributions, which one(s) will cause the posterior $P(y = 1|x)$ to have a logistic function form?
  - Gaussian
  - Poisson
  - Uniform
  - None of the above

q2_1_s: |
  - Gaussian
  - Poisson

q2_2: |
  **(b)** [3 pts] Given the following data samples (square and triangle belong to two different classes), which one(s) of the following algorithms can produce zero training error?
  <hr class="s2" />
  ![cartesian plot](/img/cs189/mt1-sp14-q2-1.png)
  - 1-nearest neighbor
  - Support vector machine
  - Logistic regression
  - Linear discriminant analysis

q2_2_s: |
  - 1-nearest neighbor
  - Support vector machine
  - Logistic regression
  - Linear discriminant analysis

q2_3: |
  **(c)** [3 pts] The following diagrams show the iso-probability contours for two different 2D Gaussian distributions. On the left side, the data $∼ N (\mathbf{0, I})$ where $\mathbf{I}$ is the identity matrix. The right side has the same set of contour levels as left side. What is the mean and covariance matrix for the right side’s multivariate Gaussian distribution?
  <hr class="s2" />
  ![isocontours](/img/cs189/mt1-sp14-q2-2.png)
  - $\mu = [0, 0]^T, \qquad \sum = \left[ \begin{array}{c c} 1 & 0 \\\\ 0 & 1 \\\\ \end{array} \right]$
  - $\mu = [0, 1]^T, \qquad \sum = \left[ \begin{array}{c c} 1 & 0 \\\\ 0 & 1 \\\\ \end{array} \right]$
  - $\mu = [0, 1]^T, \qquad \sum = \left[ \begin{array}{c c} 4 & 0 \\\\ 0 & 0.25 \\\\ \end{array} \right]$
  - $\mu = [0, 1]^T, \qquad \sum = \left[ \begin{array}{c c} 2 & 0 \\\\ 0 & 0.5 \\\\ \end{array} \right]$

q2_3_s: |
  - $\mu = [0, 1]^T, \qquad \sum = \left[ \begin{array}{c c} 4 & 0 \\\\ 0 & 0.25 \\\\ \end{array} \right]$

q2_4: |
  **(d)** [3 pts] Given the following data samples (square and triangle mean two classes), which one(s) of the following kernels can we use in SVM to separate the two classes?
  <hr class="s2" />
  ![cartesian plot](/img/cs189/mt1-sp14-q2-3.png)
  - Linear kernel
  - Polynomial kernel
  - Gaussian RBF (radial basis function) kernel
  - None of the above

q2_4_s: |
  - Polynomial kernel
  - Gaussian RBF (radial basis function) kernel

q2_5: |
  **(e)** [3 pts] Consider the following plots of the contours of the unregularized error function along with the constraint region. What regularization term is used in this case?
  ![contour plot](/img/cs189/mt1-sp14-q2-4.png)
  - $L_2$
  - $L_1$
  - $L_{\infty}$
  - None of the above

q2_5_s: |
  - $L_1$

q2_6: |
  **(f)** [3 pts] Suppose we have a covariance matrix
  $$\sum = \left[ \begin{array}{c c} 5 & a \\\\ a & 4 \end{array} \right]$$
  What is the set of values that $a$ can take on such that $\sum$ is a valid covariance matrix?
  - $a \in \mathbb{R}$
  - $-\sqrt{20} \le a \le \sqrt{20}$
  - $a \ge 0$
  - $-\sqrt{20} \lt a \lt \sqrt{20}$

q2_6_s: |
  - $-\sqrt{20} \le a \le \sqrt{20}$

q2_7: |
  **(g)** [3 pts] The soft margin SVM formulation is as follows:
  $$\text{min}\frac{1}{2}\mathbf{w^Tw} + C\sum_{i=1}^{N}ξ_i \\\\
  \begin{align} \text{subject to} \quad y_i(\mathbf{w^Tx_i} + b) & \ge 1 - ξ_i \quad \forall i \\\\
  ξ_i & \ge 0 \quad \forall i \end{align}$$
  What is the behavior of the width of the margin $(\frac{2}{\lVert w \rVert})$ as $C \rightarrow 0$?
  - Behaves like hard margin
  - Goes to infinity
  - Goes to zero
  - None of the above

q2_7_s: |
  - Goes to infinity

q2_8: |
  **(h)** [3 pts] In Homework 4, you fit a logistic regression model on spam and ham data for a Kaggle Competition. Assume you had a very good score on the public test set, but when the GSIs ran your model on a private test set, your score dropped a lot. This is likely because you overfitted by submitting multiple times and changing the following between submissions:
  - $\lambda$, your penalty turn
  - $η$, your step size
  - $\varepsilon$, your convergence criterion
  - Fixing a random bug

q2_8_s: |
  - $\lambda$, your penalty turn
  - $η$, your step size
  - $\varepsilon$, your convergence criterion
  - Fixing a random bug

q2_9: |
  **(i)** [0 pts] **BONUS QUESTION** (Answer this only if you have time and are confident of your other answers because this is not extra points.)
  <hr class="s2" />
  We have constructed the multiple choice problems such that every false positive will incur some negative penalty. For one of these multiple choice problems, given that there are $p$ points, $r$ correct answers, and $k$ choices, what is the formula for the penalty such that the expected value of random guessing is equal to 0? (You may assume $k \gt r$)

q2_9_s: |
  $$\frac{p}{k - r}$$

q3_1: |
  ## Q3. [8 pts] Decision Theory
  Consider the following generative model for a 2-class classification problem, in which the class conditionals are Bernoulli distributions:
  $$\begin{align}p(\omega_1) &= \pi \\\\
    p(\omega_2) &= 1 - \pi \\\\
    x|\omega_1 &= \begin{cases} 1 \quad \text{with probability 0.5} \\\\ 0 \quad \text{with probability 0.5} \end{cases} \\\\
    x|\omega_2 &= \begin{cases} 1 \quad \text{with probability 0.5} \\\\ 0 \quad \text{with probability 0.5} \end{cases} \\\\
    \end{align}$$
  Assume the loss matrix
  <hr class="s2" />
  <p align="center">![loss matrix](/img/cs189/mt1-sp14-q3-1.png)</p>
  <hr class="s2" />
  **(a)** [8 pts] Give a condition in terms of $\lambda_{12}, \lambda_{21}$, and $\pi$ that determines when class 1 should always be chosen as the minimum-risk class.

q3_1_s: |
  Based on Bayes' Rule, the posterior probability of $P(w_i|x)$ is
  $$P(w_1|x) = \frac{P(x|w_1)P(w_1)}{P(x)} = \frac{\frac{1}{2}\pi}{P(x)}$$
  $$P(w_2|x) = \frac{P(x|w_2)P(w_2)}{P(x)} = \frac{\frac{1}{2}(1 - \pi)}{P(x)}$$
  Risk for predicting class 1 is
  $$R(\alpha_1|x) = \lambda_{11}P(w_1|x) + \lambda_{12}P(w_2|x) = \frac{\lambda_{12}(1 - \pi)}{2P(x)}$$
  Risk for predicting class 2 is
  $$R(\alpha_2|x) = \lambda_{21}P(w_1|x) + \lambda_{22}P(w_2|x) = \frac{\lambda_{21}\pi}{2P(x)}$$
  Choose class 1 when $R(\alpha_1|x) < R(\alpha_2|x)$, i.e. $\frac{\lambda_{12}(1 - \pi)}{2P(x)} < \frac{\lambda_{21}\pi}{2P(x)}$, which is
  $$\lambda_{12}(1 - \pi) < \lambda_{21} \pi$$

q4_1: |
  ## Q4. [14 pts] Kernels
  **(a)** [6 pts] Let $k_1$ and $k_2$ be (valid) kernels; that is, $k_1(\mathbf{x, y}) = \Phi_1(\mathbf{x})^T\Phi_1(\mathbf{y})$ and $k_2(\mathbf{x, y}) = \Phi_2(\mathbf{x})^T\Phi_2(\mathbf{y})$.
  <hr class="s1" />
  Show that $k = k_1 + k_2$ is a valid kernel by explicitly constructing a corresponding feature mapping $\Phi(\mathbf{z})$.

q4_1_s: |
  $$\begin{align} k(x, y) &= k_1(x, y) + k_2(x, y) \\\\
  &= \mathcal{\Phi}_1(\mathbf{x})^T\Phi_1(\mathbf{y}) + \Phi_2(\mathbf{x})^T\Phi_2(\mathbf{y}) \\\\
  &= [\Phi_1(\mathbf{x})\Phi_2(\mathbf{x})]^T[\Phi_1(\mathbf{x})\Phi_2(\mathbf{x})] \end{align}$$
  If we let $\phi(\mathbf{z}) = [\Phi_1(\mathbf{x}) \; \Phi_2(\mathbf{x})]$, then we have $k(x, y) = \phi(\mathbf{z})^T\phi(\mathbf{z})$. Therefore, $k = k_1 + k_2$ is a valid kernel.

q4_2: |
  **(b)** [8 pts] The polynomial kernel is defined to be
  $$k(\mathbf{x, y}) = (\mathbf{x^Ty} + c)^d$$
  where $\mathbf{x, y} \in \mathbb{R}^n$ and $c \ge 0$. When we take $d = 2$, this kernel is called the quadratic kernel. Find the feature mapping $\Phi(\mathbf{z})$ that corresponds to the quadratic kernel.

q4_2_s: |
  First we expand the dot product inside, and square the entire sum. We will get a sum of the squares of the components and a sum of the cross products.
  $$(\mathbf{x^Ty} + c)^2 = (c + \sum_{i=1}^{n}x_iy_i)^2 \\\\
  = c^2 + \sum_{i=1}^{n}x_i^2y_i^2 + \sum_{i=2}^{n}\sum_{j=1}^{i-1}2x_iy_ix_jy_j + \sum_{i=1}^{n}2x_iy_ic$$
  Pulling this sum into a dot product of $x$ components and $y$ components, we have
  $$\Phi(x) = [c, x_1^2, ..., x_n^2, \sqrt{2}x_1x_2, ..., \sqrt{2}x_1x_n, \sqrt{2}x_2x_3, ..., \sqrt{2}x_{n-1}x_n, \sqrt{2c}x_1, ..., \sqrt{2c}x_n]$$
  In this feature mapping, we have $c$, the squared components of the vector $\mathbf{x}$, $\sqrt{2}$ multiplied by all of the cross terms, and $\sqrt{2c}$ multiplied by all of the components.

q5_1: |
  ## Q5. [8 pts] L2-Regularized Linear Regression with Newton's Method
  Recall that the objective function for L2- regularized linear regression is
  $$J(\mathbf{w}) = {\lVert X\mathbf{w} - \mathbf{y} \rVert}_2^2 + \lambda{\lVert \mathbf{w} \rVert}_2^2$$
  where $X$ is the design matrix (the rows of $X$ are the data points).
  <hr class="s2" />
  The global minimizer of $J$ is given by:
  $$\mathbf{w}^{\*} = (X^TX + \lambda I)^{-1}X^T\mathbf{y}$$
  **(a)** [8 pts] Consider running Newton's method to minimize $J$.
  <hr class="s1" />
  Let $\mathbf{w}_0$ be an arbitrary initial guess for Newton's method. Show that $\mathbf{w}_1$, the value of the weights after one Newton step, is equal to $\mathbf{w}^{\*}$.

q5_1_s: |
  Recall that Newton's Method for Optimization is
  $$w_1 = w_0 - [H(J(w))]^{-1} \nabla_w J(w)$$
  Solving for the gradient, we have:
  $$\nabla_w J(w) = 2X^TXw - 2X^TY + 2\lambda w = 2[(X^TX + \lambda I)w - X^TY]$$
  Solving for the Hessian, we have:
  $$H(J(w)) = \nabla_w^2 J(w) = 2X^TX + 2\lambda I = 2(X^TX + \lambda I)$$
  We initialize $w_0$ to some value. Note that this won't matter. Plugging this in, we have
  $$\begin{align} w_1 &= w_0 - (X^TX + \lambda I)^{-1}2^{-1}2[(X^TX + \lambda I)w_0 - X^TY] \\\\
  &= w_0 - (X^TX + \lambda I)^{-1}(X^TX + \lambda I)w_0 + (X^TX + \lambda I)^{-1}X^TY \\\\
  &= w_0 - w_0 + (X^TX + \lambda I)^{-1} X^T Y \\\\
  &= (X^TX + \lambda I)^{-1}X^TY \end{align}$$
  Thus, $w_1 = w^{\*}$.

q6_1: |
  ## Q6. [8 pts] Maximum Likelihood Estimation
  **(a)** [8 pts] Let $x_1, x_2, ..., x_n$ be independent samples from the following distribution:
  $$P(x|\theta) = \theta x^{-\theta - 1} \text{ where } \theta > 1, x \ge 1$$
  Find the maximum likelihood estimator of $\theta$.

q6_1_s: |
  $$L(\theta|x_1, x_2, ..., x_n) = \prod_{i=1}^{n}\theta x_i^{-\theta - 1} = \theta^n \prod_{i=1}^{n} x_i^{-\theta - 1} \\\\
  \text{ln} \; L(\theta|x_1, x_2, ..., x_n) = n\;\text{ln}\theta - (\theta + 1)\sum_{i=1}^{n}\text{ln}x_i \\\\
  \frac{\partial \text{ln} L}{\partial \theta} = \frac{n}{\theta} - \sum_{i=1}^{n}\text{ln} \; x_i = 0 \\\\
  \theta_{mle} = \frac{n}{\sum_{i=1}^{n} \text{ln}\; x_i}$$
  Since $\theta > 1$, any $\theta_{mle} \le 1$ has a zero probability of generating any data, so our best estimate of $\theta$ when $\theta_{mle} \le 1$ is $\theta_{mle} = 1$. Therefore, the final answer is $\theta_{mle} = \text{max}(1, \frac{n}{\sum_{i=1}^{n}\text{ln}\;x_i})$.
  <hr class="s1" />
  However, we will still accept $\theta_{mle} = \frac{n}{\sum_{i=1}^{n}\text{ln}\;x_i}$.

q7_1: |
  ## Q7. [13 pts] Affine Transformations of Random Variables
  Let $\mathbf{X}$ be a $d$-dimensional random vector with mean $\mathbf{\mu}$ and covariance matrix $Σ$. Let $\mathbf{Y} = A\mathbf{X} + \mathbf{b}$, where $A$ is a $n \text{ x } d$ matrix and $\mathbf{b}$ is a $n$-dimensional vector.
  <hr class="s2" />
  **(a)** [6 pts] Show that the mean of $\mathbf{Y}$ is $A\mathbf{\mu} + \mathbf{b}$.

q7_1_s: |
  $$\mathbb{E}(\mathbf{Y}) = \mathbb{E}(A\mathbf{X} + b) = \mathbb{E}(A\mathbf{X}) + \mathbb{E}(\mathbf{b}) = A\mathbb{E}(\mathbf{X}) + \mathbf{b} = A\mathbf{\mu} + \mathbf{b}$$

q7_2: |
  **(b)** [7 pts] Show that the covariance matrix of $\mathbf{Y}$ is $A Σ A^T$.

q7_2_s: |
  $$\begin{align} \text{Cov}(\mathbf{Y}) &= \mathbb{E}((\mathbf{Y} - \mathbb{E}\mathbf{Y})(\mathbf{Y} - \mathbb{E}\mathbf{Y})^T) \\\\
  &= \mathbb{E}((A\mathbf{X} + \mathbf{b} - A\mathbf{\mu} - \mathbf{b})(A\mathbf{X} + \mathbf{b} - A\mathbf{\mu} - \mathbf{b})^T) \\\\
  &= \mathbb{E}((A\mathbf{X} - A\mathbf{\mu})(A\mathbf{X} - A\mathbf{\mu})^T) \\\\
  &= \mathbb{E}(A(\mathbf{X} - \mathbf{\mu})(\mathbf{X} - \mathbf{\mu})^TA^T) \\\\
  &= A\mathbb{E}((\mathbf{X} - \mathbf{\mu})(\mathbf{X} - \mathbf{\mu})^T)A^T \\\\
  &= AΣ A^T \end{align}$$

q8_1: |
  ## Q8. [15 pts] Generative Models
  Consider a generative classification model for $K$ classes defined by the following:
  - Prior class probabilities: $P(C_k) = \pi_k \quad k = 1, ..., K$
  - General class-conditional densities: $P(\mathbf{x}|C_k) \quad k = 1, ..., K$

  Suppose we are given training data $\\{(\mathbf{x}_n, \mathbf{y}_n)\\}_{n=1}^N$ drawn independently from this model.
  <hr class="s2" />
  The labels $\mathbf{y}_i$ are "one-of-$K$" vectors; that is, $K$-dimensional vectors of all 0's except for a single 1 at the element corresponding to the class. For example, if $K = 4$ and the true label of $\mathbf{x}_i$ is class 2, then
  $$\mathbf{y}_i = {\left[ \begin{array}{c c c c} 0 & 1 & 0 & 0 \end{array} \right]}^T$$
  **(a)** [5 pts] Write the log likelihood of the data set. You may use $y_{ij}$ to denote the $j^{th}$ element of $\mathbf{y}_i$.

q8_1_s: |
  The probability of one data point is
  $$\mathbb{P}(\mathbf{x, y}) = \mathbb{P}(\mathbf{x|y})\mathbb{P}(\mathbf{y}) = \prod_{k=1}^{K}(\mathbb{P}(\mathbf{x}|C_k)\pi_k)^{\mathbf{y_k}}$$
  We denote the parameters of this model as $\theta$. The independent samples allow us to take a product over the data points.
  $$\mathcal{L}(\theta) = \prod_{n=1}^{N}\prod_{k=1}^{K}(\mathbb{P}(\mathbf{x}_n|C_k)\pi_k)^{\mathbf{y}_{n,k}}$$
  Thus,
  $$\mathcal{l}(\theta) = \prod_{n=1}^{N}\prod_{k=1}^{K}y_{n,k}[\text{log}(\mathbb{P}(\mathbf{x}_n|C_k)) + \text{log}\pi_k]$$

q8_2: |
  **(b)** [10 pts] What are the maximum likelihood estimates of the prior probabilities?
  <hr class="s1" />
  (Hint: Remember to use Lagrange multipliers!)

q8_2_s: |
  We want to maximize the log likelihood subject to the constraint that $\sum_{k=1}^{K}\pi_k = 1$. Thus, we must introduce Lagrange Multipliers. The parameters we are about here are the $\pi_k$'s. Here is the Lagrangian:
  $$\mathscr{L}(\pi, \lambda) = \sum_{n=1}^{N}\sum_{k=1}^{K}y_{n,k}[\text{log}\;(\mathbb{P}(\mathbf{x}_n|C_k) + \text{log}\;\pi_k] + \lambda \left( \sum_{k-1}^{K}\pi_k - 1 \right)$$
  Taking the derivative with respect to $\pi_k$ and setting it to 0, we have
  $$\frac{\partial}{\partial\pi_k}\mathscr{L}(\pi, \lambda) = \frac{1}{\pi_k}\sum_{n=1}^{N}y_{n,k} + \lambda = 0 \implies \pi_k = -\frac{1}{\lambda} \sum_{n=1}^{N}y_{n,k} = -\frac{N_k}{\lambda}$$
  where $N_k$ is the number of data points whose label is class $k$. Taking the derivative with respect to $\lambda$, we have
  $$\frac{\partial}{\partial\lambda}\mathscr{L}(\pi, \lambda) = \sum_{k=1}^{K}\pi_k - 1 = 0 \implies \sum_{k=1}^{K} \pi_k = 1$$
  We can plug in all of our values of the $\pi_k$'s into the constraint, giving us the value of $\lambda$:
  $$\sum_{k=1}^{K} \pi_k = \sum_{k=1}^{K}-\frac{N_k}{\lambda} = -\frac{N}{\lambda} = 1 \implies \lambda = -N$$
  After having solved for $\lambda$, we can just plug this back into our other equations to solve for our $\pi_k$'s. Thus, we have that the maximum likelihood estimates of the prior probabilities are
  $$\pi_k = \frac{N_k}{N}$$
