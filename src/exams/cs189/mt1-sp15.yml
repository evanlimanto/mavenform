course: 'cs189'
ref: 'mt1-sp15'

type: 'mt1'
term: 'sp15'
prof: 'Efros, Bartlett'

questions: {
  'q1': 'True or False',
  'q2': 'Multiple Choice',
  'q3': 'Parameter Estimation',
  'q4': 'Dual Solution for Ridge Regression',
  'q5': 'Regularization and Priors for Linear Regression',
}

parts: {
  'q1': 13,
  'q2': 12,
  'q3': 2,
  'q4': 3,
  'q5': 2,
}

q1_1: |
  ## Q1. [26 pts] True or False
  **(a)** [2 pts] If the data is not linearly separable, then there is no solution to the hard-margin SVM.
  - True
  - False

q1_1_s: |
  - True

q1_2: |
  **(b)** [2 pts] Logistic Regression can be used for classification.
  - True
  - False

q1_2_s: |
  - True

q1_3: |
  **(c)** [2 pts] In logistic regression, two ways to prevent $\beta$ vectors from getting too large are using a small step size and using a small regularization value.
  - True
  - False

q1_3_s: |
  - False

q1_4: |
  **(d)** [2 pts] The L2 norm is often used because it produces sparse results, as opposed to the L1 norm which does not.
  - True
  - False

q1_4_s: |
  - False

q1_5: |
  **(e)** [2 pts] For a Multivariate Gaussian, the eigenvalues of the covariance matrix are inversely proportional to the lengths of the ellipsoid axes that determine the isocontours of the density.
  - True
  - False

q1_5_s: |
  - False

q1_6: |
  **(f)** [2 pts] In a generative binary classification model where we assume the class conditionals are distributed as Poisson, and the class priors are Bernoulli, the posterior assumes a logistic form.
  - True
  - False

q1_6_s: |
  - True

q1_7: |
  **(g)** [2 pts] Maximum likelihood estimation gives us not only a point estimate, but a distribution over the parameters that we are estimating.
  - True
  - False

q1_7_s: |
  - False

q1_8: |
  **(h)** [2 pts] Penalized maximum likelihood estimators and Bayesian estimators for parameters are better used in the setting of low-dimensional data with many training examples as opposed to the setting of high-dimensional data with few training examples.
  - True
  - False

q1_8_s: |
  - False

q1_9: |
  **(i)** [2 pts] It is not a good machine learning practice to use the test set to help adjust the hyperparameters of your learning algorithm.
  - True
  - False

q1_9_s: |
  - True

q1_10: |
  **(j)** [2 pts] A symmetric positive semi-definite matrix always has nonnegative elements.
  - True
  - False

q1_10_s: |
  - False

q1_11: |
  **(k)** [2 pts] For a valid kernel function $K$, the corresponding feature mapping $\phi$ can map a finite dimensional vector into an infinite dimensional vector.
  - True
  - False

q1_11_s: |
  - True

q1_12: |
  **(l)** [2 pts] The more features that we use to represent our data, the better the learning algorithm will generalize to new data points.
  - True
  - False

q1_12_s: |
  - False

q1_13: |
  **(m)** [2 pts] A discriminative classifier explicitly models $P(Y|X)$.
  - True
  - False

q1_13_s: |
  - True

q2_1: |
  ## Q2. [36 pts] Multiple Choice
  Fill in the bubbles for **ALL correct choices**: there may be more than one correct choice, but there is always at least one correct choice. **NO partial credit**: the set of all correct answers must be checked.
  <hr class="s1" />
  **(a)** [3 pts] Which of the following algorithms can you use kernels with?
  - Support Vector Machines
  - Perceptrons
  - None of the above

q2_1_s: |
  - Support Vector Machines
  - Perceptrons

q2_2: |
  **(b)** [3 pts] Cross validation:
  - Is often used to select hyperparameters
  - Is guaranteed to prevent overfitting
  - Does nothing to prevent overfitting
  - None of the above

q2_2_s: |
  - Is often used to select hyperparameters

q2_3: |
  **(c)** [3 pts] In linear regression, L2 regularization is equivalent to imposing a:
  - Logistic prior
  - Gaussian prior
  - Laplace prior
  - Gaussian class-conditional

q2_3_s: |
  - Gaussian prior

q2_4: |
  **(d)** [3 pts] Say we have two 2-dimensional Gaussian distributions representing two different classes. Which of the following conditions will result in a linear decision boundary:
  - Same mean for both classes
  - Same covariance matrix for both classes
  - Different covariance matrix for each class
  - Linearly separable data

q2_4_s: |
  - Same covariance matrix for both classes

q2_5: |
  **(e)** [3 pts] The normal equations can be derived from:
  - Minimizing empirical risk
  - Assuming that $Y = \beta^Tx + \varepsilon$, where $\varepsilon ∼ \mathcal{N}(0, \sigma^2)$.
  - Assuming that the $P(Y|X = x)$ is distributed normally with mean $\beta^Tx$ and variance $\sigma^2$.
  - Finding a linear combination of the rows of the design matrix that minimizes the distance to our vector of labels $Y$.

q2_5_s: |
  - Minimizing empirical risk
  - Assuming that $Y = \beta^Tx + \varepsilon$, where $\varepsilon ∼ \mathcal{N}(0, \sigma^2)$.
  - Assuming that the $P(Y|X = x)$ is distributed normally with mean $\beta^Tx$ and variance $\sigma^2$.

q2_6: |
  **(f)** [3 pts] Logistic regression can be motivated from:
  - Generative models with uniform class conditionals
  - Generative models with Gaussian class conditionals
  - Log odds being equated to an affine function of $x$
  - None of the above

q2_6_s: |
  - Generative models with Gaussian class conditionals
  - Log odds being equated to an affine function of $x$

q2_7: |
  **(g)** [3 pts] The perceptron algorithm will converge:
  - If the data is linearly separable
  - Even if the data is linearly inseparable
  - As long as you initialize $\theta$ to all 0's
  - Always

q2_7_s: |
  - If the data is linearly separable

q2_8: |
  **(h)** [3 pts] Which of the following is true:
  - Newton's Method typically is more expensive to calculate than gradient descent, per iteration
  - For quadratic equations, Newton's Method typically requires fewer iterations than gradient descent
  - Gradient descent can be viewed as iteratively reweighted least squares
  - None of the above

q2_8_s: |
  - Newton's Method typically is more expensive to calculate than gradient descent, per iteration
  - For quadratic equations, Newton's Method typically requires fewer iterations than gradient descent

q2_9: |
  **(i)** [3 pts] Which of the following statements about duality and SVMs is (are) true?
  - Complementary slackness implies that every training point that is misclassified by a soft-margin SVM is a support vector.
  - When we solve the SVM with the dual problem, we need only the dot product of $x_i, x_j$ for all $i, j$, and no other information about the $x_i$.
  - We use Lagrange multipliers in an optimization problem with inequality ($\le$) constraints.
  - None of the above.

q2_9_s: |
  - Complementary slackness implies that every training point that is misclassified by a soft-margin SVM is a support vector.
  - When we solve the SVM with the dual problem, we need only the dot product of $x_i, x_j$ for all $i, j$, and no other information about the $x_i$.
  - We use Lagrange multipliers in an optimization problem with inequality ($\le$) constraints.

q2_10: |
  **(j)** [3 pts] Which of the following distance metrics can be computed exclusively with inner products, assuming $\phi(x)$ and $\phi(y)$ are feature mappings of $x$ and $y$, respectively?
  - $\phi(x) - \phi(y)$
  - ${\lVert \phi(x) - \phi(y) \rVert}_1$
  - ${\lVert \phi(x) - \phi(y) \rVert}_2^2$
  - None of the above

q2_10_s: |
  - ${\lVert \phi(x) - \phi(y) \rVert}_2^2$

q2_11: |
  **(k)** [3 pts] Strong duality holds for:
  - Hard Margin SVM
  - Soft Margin SVM
  - Constrainted optimization porblems in general
  - None of the above

q2_11_s: |
  - Hard Margin SVM
  - Soft Margin SVM

q2_12: |
  **(l)** [3 pts] Which of the following facts about the 'C' in SVMs is (are) true?
  - As C approaches 0, the soft margin SVM is equal to the hard margin SVM
  - C can be negative, as long as each of the slack variables are nonnegative
  - A larger C tends to create a larger margin
  - None of the above

q2_12_s: |
  - None of the above

q3_1: |
  ## Q3. [10 pts] Parameter Estimation
  In this problem, we have $n$ trials with $k$ possible types of outcomes $\\{1, 2, ..., k\\}$. Suppose we observe $X_1, ..., X_k$ where each $X_i$ is the number of outcomes of type $i$. If $p_i$ refers to the probability that a trial has outcome $i$, then $(X_1, ..., X_k)$ is said to have multinomial distribution with paramters $p_1, ..., p_k$, denoted $(X_1, ..., X_k) ∼ Multinomial(p_1, ..., p_k)$. It may be useful to know that the probability mass function of the multinomial distribution is given as follows.
  $$P(X_1 = x_1, ..., X_k = x_k) = \frac{n!}{x_1!x_2!...x_k!}p_1^{X_1}\cdots p_k^{X_k}$$
  We want to find the maximum likelihood estimators for $p_1, ..., p_k$. You may assume that $p_i \gt 0$ for all $i$.
  <hr class="s2" />
  **(a)** [4 pts] What is the log-likelihood function, $l(p_1, ..., p_k | X_1, ..., X_k)$?

q3_1_s: |
  The likelihood function is given as follows:
  $$L(p_1, ..., p_k | X_1, ... X_k) = P(X_1, ..., X_k | p_1, ..., p_k) = \frac{n!}{X_1!X_2!...X_k!}p_1^{X_1} \cdots p_k^{X_k}$$
  Therefore, the log-likelihood is given as follows:
  $$l(p_1, ..., p_k | X_1, ... X_k) = \text{log}(n!) - \sum_{i=1}^{k}\text{log}(X_i!) + \sum_{i=1}^{n}X_i \text{log}p_i$$

q3_2: |
  **(b)** [6 pts] you might notice that unconstrained maximization of this function leads to an answer in which we set each $p_i = \infty$. But this is wrong. We must add a constraint such that the probabilities sum up to 1. Now, we have the following optimization problem.
  $$max_{p_1,...,p_k}l(p_1,...,p_k|X_1,...,X_k)$$
  $$\text{s.t.}\sum_{i=1}^{k}p_i = 1$$
  Recall that we can use the method of Lagrange multipliers to solve an optimization problem with equality constraints. Using this method, find the maximum likelihood estimators for $p_1,...,p_k$.

q3_2_s: |
  By applying the Method of Lagrange Multipliers, we get the following Lagrangian.
  $$L(p_1,...,p_k,\lambda) = \text{log}(n!) - \sum_{i=1}^{k}\text{log}(X_i!) + \sum_{i=1}^{n}X_i \text{log}p_i + \lambda(1 - \sum_{i=1}^{k}p_i)$$
  We take the derivative with respect to each $p_i$ and $\lambda$ and set it equal to 0.
  $$\frac{\partial L}{\partial p_i} = \frac{X_i}{p_i} - \lambda = 0$$
  $$\frac{\partial L}{\partial \lambda} = 1 - \sum_{i=1}^{k}p_i = 0$$
  Solving for $p_i$, we get the MLE.
  $$\hat{p_i}_{MLE} = \frac{X_i}{n}$$

q4_1: |
  ## Q4. [8 pts] Dual Solution for Ridge Regression
  Recall that ridge regression minimizes the objective function:
  $$L(w) = {\lVert Xw - y \rVert}_2^2 + \lambda {\lVert w \rVert}_2^2$$
  where $X$ is an $n$-by-$d$ design matrix, $w$ is a $d$-dimensional vector and $y$ is a $n$-dimensional vector. We already know that the function $L(w)$ is minimized by
  $$w^{\*} = (X^X + \lambda I)^{-1}X^Ty \text{.}$$
  Alternatively, the minimizer can be represented by a linear combination of the design matrix rows. That is, there exists a $n$-dimensional vector $\alpha^{\*}$ such that the objective function $L(w)$ is minimized by $w^{\*} = X^T \alpha^{\*}$. The vector $\alpha^{\*}$ is called the dual solution to the linear regression problem.
  <hr class="s2" />
  **(a)** [2 pts] Using the relation $w = X^T\alpha$, define the objective function $L$ in terms of $\alpha$.

q4_1_s: |
  $$L(X^T\alpha) = {\lVert XX^T\alpha - y \rVert}_2^2 + \lambda {\lVert X^T \alpha \rVert}_2^2$$

q4_2: |
  **(b)** [3 pts] Show that $\alpha^{\*} = (XX^T + \lambda I)^{-1} y$ is a dual solution.

q4_2_s: |
  Using the relation $w = X^T \alpha$, we define the objective function in terms of $\alpha$
  $$\mathcal{l}(a) = L(X^T \alpha) = {\lVert XX^T\alpha - y \rVert}_2^2 + \lambda{\lVert X^T \alpha \rVert}_2^2 \text{.}$$
  Since $X\alpha^{\*}$ is the minimizer of $L(w)$, the vector $\alpha^{\*}$ should be the minimizer of function $\mathcal{l}(\alpha)$. By calculating the gradient of function $\mathcal{l}(\alpha)$ and make it equal to zero, we get
  $$XX^T(XX^T\alpha - y + \lambda \alpha) = 0$$
  Since $\mathcal{l}(\alpha)$ is a convex function, every solution to the above equation is a minimizer of the objective function.
  <hr class="s1" />
  Since $\alpha^{\*} = (XX^T + \lambda I)^{-1}y$ is one solution, it establishes the claim.
  <hr class="s2" />
  Alternate Proof: to verify that $\alpha^{\*}$ is a dual solution, it suffices to verify that $w^{\*} = X^T\alpha^{\*}$, or equivalently
  $$\begin{align} (X^TX + \lambda I)^{-1}X^Ty &= X^T(XX^T + \lambda I)^{-1}y \\\\
  \iff X^Ty &= (X^TX + \lambda I)X^T(XX^T + \lambda I)^{-1}y \\\\
  \iff X^Ty &= (X^TXX^T + \lambda X^T)(XX^T + \lambda I)^{-1}y \\\\
  \iff X^Ty &= X^T(XX^T + \lambda I)(XX^T + \lambda I)^{-1}y \\\\
  \iff X^Ty &= X^Ty \end{align}$$
  which completes the proof.

q4_3: |
  **(c)** [3 pts] To make the solution in question (b) well-defined, the matrix $XX^T + \lambda I$ has to be an invertible matrix. Assuming $\lambda \gt 0$, show that $XX^T + \lambda I$ is an invertible matrix. (Hint: positive definite matrices are invertible)

q4_3_s: |
  Since all positive definite matrices are invertible, it suffices to show that the matrix $XX^T + \lambda I$ is positive definite. For any non-zero vector $v$, we have
  $$v^T(XX^T + \lambda I)v = v^TXX^Tv + \lambda v^Tv = (X^Tv)^T(X^Tv) + \lambda v^Tv = {\lVert X^Tv \rVert}_2^2 + \lambda {\lVert v \rVert}_2^2$$
  Since $v \ne 0$, we have ${\lVert X^Tv \rVert}_2^2 \ge 0$ and $\lambda {\lVert v \rVert}_2^2 \ge 0$, which implies that $v^T(XX^T + \lambda I)v \gt 0$. It establishes that the matrix is positive definite.

q5_1: |
  ## Q5. [8 pts] Regularization and Priors for Linear Regression
  Linear regression is a model of the form $P(y|\mathbf{x}) ∼ \mathcal{N}(\mathbf{w^Tx}, \sigma^2)$, where $\mathbf{w}$ is a $d$-dimensional vector. Recall that in ridge regression, we add an $\mathcal{l}_2$ regularization term to our least squares objective function to prevent overfitting, so that our loss function becomes:
  $$J(\mathbf{w}) = \sum_{i=1}^{n}(Y_i = \mathbf{w^TX_i})^2 + \lambda \mathbf{w^Tw} \\qquad (\*)$$
  We can arrive at the same objective function in a Bayesian setting, if we consider a MAP (maximum a posteriori probability) estimate, where $\mathbf{w}$ has the prior distribution $\mathcal{N}(0, f(\lambda, \sigma)I)$.
  <hr class="s2" />
  **(a)** [3 pts] What is the conditional desntiy of $w$ given the data?

q5_1_s: |
  $$P(w|\mathbf{X_i},Y_i) ∝ (\prod_{i=1}^{n}\mathcal{N}(Y_i|\mathbf{w^TX_i},\sigma^2)) \cdot P(\mathbf{w}) = (\prod_{i=1}^{n}\mathcal{N}(Y_i|\mathbf{w^TX_i}, \sigma^2)) \cdot \prod_{j=1}^{d} P(w_j)$$

q5_2: |
  **(b)** [5 pts] What $f(\lambda, \sigma)$ makes this MAP estimate the same as the solution to ($\*$)?

q5_2_s: |
  Taking logs, we want to maximize
  $$\mathcal{l}(\mathbf{w}) = \sum_{i=1}^{n}\text{log}\;\mathcal{N}(Y_i|\mathbf{w^TX_i}, \sigma^2) + \sum_{j=1}^{d}\text{log}\;P(w_j) \\\\
  = \sum_{i=1}^{n}\text{log}\;(\frac{1}{\sqrt{2\pi}\sigma}\text{exp}(-\frac{(Y_i - \mathbf{w^TX_i})^2}{2\sigma^2})) + \sum_{j=1}^{d}\text{log}\;(\frac{1}{\sqrt{2\pi f(\lambda, \sigma)}} \text{exp}(\frac{-w_j^2}{2 f(\lambda, \sigma)})) \\\\
  = - \sum_{i=1}^{n}\frac{(Y_i - \mathbf{w^TX_i})^2}{2\sigma^2} + \frac{-\sum_{j=1}^{d} w_j^2}{2 f(\lambda, \sigma)} + n\; \text{log}\;(\frac{1}{\sqrt{2\pi}\sigma}) + d\; \text{log}\;(\frac{1}{2 f(\lambda, \sigma)})\text{,}$$
  so it is equivalent to minimizing the following function
  $$\begin{align} J(\mathbf{w}) &= \sum_{i=1}^{n}(Y_i - \mathbf{w^TX_i})^2 + \frac{\sigma^2}{f(\lambda, \sigma)}\sum_{j=1}^{d}w_j^2 \\\\
  &= \sum_{i=1}^{n}(Y_i - \mathbf{w^TX_i})^2 + \lambda \mathbf{w^Tw}\text{,} \end{align}$$
  hence $f(\lambda, \sigma) = \frac{\sigma^2}{\lambda}$.
